{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "We will build a preprocessing pipeline in order to shape the documents for training models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 09:58:02.548237: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from config import RAW_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(RAW_DATA_PATH, 'CADA-2022-05-31.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "documents = df['Avis'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Basic tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "There are different ways to encode documents into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's use the SpacY tokenizer with default settings on French vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = French()\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def document_preprocessing(document, tokenizer):\n",
    "    \"\"\"Normalize documents with some preprocessing tasks\"\"\"\n",
    "    \n",
    "    # split the document into tokens\n",
    "    tokens = tokenizer(str(document).lower())\n",
    "    \n",
    "    # remove stopwords and punctuation\n",
    "    tokens = [tk.text.replace('.','') for tk in tokens if\n",
    "                  tk.is_stop is False and\n",
    "                  tk.is_punct is False\n",
    "             ]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "document_tokens = []\n",
    "\n",
    "for doc in documents:\n",
    "    token_list = document_preprocessing(document=doc, tokenizer=tokenizer)\n",
    "    document_tokens.append(' '.join(token_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"commission d'accès documents administratifs examiné séance 3 mars 1984 demande l'avez saisie lettre 21 décembre 1983 \\n\\n commission émis avis défavorable communication dossier d'enquête relatif refus admission qualité d'élève-officier réserve interprète chiffre marine, motif qu'elle porterait atteinte secret défense nationale, exception prévue l'article 6 loi 17 juillet 1978\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to map documents into a feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a document as a vector of size `N` which is the vocabulary size.\n",
    "\n",
    "We encode all tokens of the document into this vector.\n",
    "\n",
    "The vector values are based on TFIDF approach:\n",
    "* The numerator is the token frequency within the document\n",
    "* The denominator is the token frequency within the whole collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    max_df=10000,  # ignore tokens that appear more than X times in the document collection\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    max_features=10000,   # capping on vocabulary size\n",
    ")\n",
    "vectorizer = vectorizer.fit(document_tokens)\n",
    "\n",
    "X = vectorizer.transform(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: 48746 documents, 10000 vocabulary size\n"
     ]
    }
   ],
   "source": [
    "print('Matrix size:', X.shape[0], 'documents,', X.shape[1], 'vocabulary size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<48746x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3956017 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the distance matrix, which represents the distance between documents.\n",
    "\n",
    "The computation cost is quadratic. Let's take the first 15k documents for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRUNCATION = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = cosine_similarity(X[:DATASET_TRUNCATION,:], dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15000x15000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 201824095 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Let's find documents that look alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, MiniBatchSparsePCA\n",
    "\n",
    "pca = MiniBatchSparsePCA(n_components=100, batch_size=50)\n",
    "\n",
    "pca = pca.fit(distance_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_distance_matrix = pca.transform(distance_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 100)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_distance_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(min_samples=10)\n",
    "\n",
    "clustering = clustering.fit(reduced_distance_matrix)\n",
    "\n",
    "labels = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_labels'] = list(labels) + [-1] * (len(df) - DATASET_TRUNCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1     48168\n",
       " 8       191\n",
       " 16       77\n",
       " 2        48\n",
       " 4        44\n",
       " 15       34\n",
       " 10       32\n",
       " 9        21\n",
       " 5        18\n",
       " 12       16\n",
       " 13       15\n",
       " 11       14\n",
       " 0        13\n",
       " 3        12\n",
       " 6        12\n",
       " 7        11\n",
       " 1        10\n",
       " 14       10\n",
       "Name: cluster_labels, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cluster_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the documents of the same cluster through the lens of the common tokens.\n",
    "\n",
    "Here we care about the most discriminative tokens i.e. the ones that are especially present in these documents\n",
    "relative to the others.\n",
    "\n",
    "To do that, we look at the tokens with highest TFIDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10000)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(doc).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('commission', 13),\n",
       " ('\\r\\n\\r\\n', 8),\n",
       " ('\\r\\n', 7),\n",
       " ('dispositions', 7),\n",
       " ('documents', 6),\n",
       " ('demande', 6),\n",
       " ('titre', 6),\n",
       " ('l', 6),\n",
       " ('informations', 6),\n",
       " ('météo', 5),\n",
       " ('france', 5),\n",
       " ('code', 5),\n",
       " (\"l'environnement\", 5),\n",
       " ('données', 4),\n",
       " (\"l'ensemble\", 4),\n",
       " ('124-1', 4),\n",
       " ('droit', 4),\n",
       " ('relatives', 4),\n",
       " (\"l'environnement,\", 4),\n",
       " ('xxx', 3),\n",
       " (\"d'accès\", 3),\n",
       " ('courrier', 3),\n",
       " ('président-directeur', 3),\n",
       " ('général', 3),\n",
       " ('rapport', 3),\n",
       " ('interministérielle', 3),\n",
       " ('sécheresse', 3),\n",
       " ('2009', 3),\n",
       " ('exploitées', 3),\n",
       " ('demandes', 3)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [doc for idx, doc in enumerate(document_tokens[:DATASET_TRUNCATION]) if labels[idx]==1][0]\n",
    "\n",
    "doc = doc.split(\" \")\n",
    "\n",
    "Counter(doc).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('commission', 13),\n",
       " ('\\r\\n\\r\\n', 8),\n",
       " ('dispositions', 7),\n",
       " ('documents', 6),\n",
       " ('demande', 6),\n",
       " ('\\xa0', 6),\n",
       " ('\\r\\n', 6),\n",
       " ('titre', 6),\n",
       " ('l', 6),\n",
       " ('informations', 6),\n",
       " ('météo', 5),\n",
       " ('france', 5),\n",
       " ('code', 5),\n",
       " (\"l'environnement\", 5),\n",
       " ('données', 4),\n",
       " (\"l'ensemble\", 4),\n",
       " ('124-1', 4),\n",
       " ('droit', 4),\n",
       " ('relatives', 4),\n",
       " (\"l'environnement,\", 4),\n",
       " ('xxx', 3),\n",
       " (\"d'accès\", 3),\n",
       " ('courrier', 3),\n",
       " ('président-directeur', 3),\n",
       " ('général', 3),\n",
       " ('rapport', 3),\n",
       " ('interministérielle', 3),\n",
       " ('sécheresse', 3),\n",
       " ('2009', 3),\n",
       " ('exploitées', 3)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [doc for idx, doc in enumerate(document_tokens[:DATASET_TRUNCATION]) if labels[idx]==1][1]\n",
    "\n",
    "doc = doc.split(\" \")\n",
    "\n",
    "Counter(doc).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.split(\"oui sncf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
